---
title:  'Computational Musicology'
author: 'Pien Zwart'
date:   'February--March 2020'
output: 
    flexdashboard::flex_dashboard:
        storyboard: true
        theme: readable
        highlight: monochrome
        
        
        
        
---

```{r setup}
# In order to use these packages, we need to install flexdashboard, plotly, and Cairo.
library(tidyverse)
library(plotly)
library(spotifyr)
source('spotify.R')
library(compmus)
```

### Pien VS. Pim

For my corpus, I would like to compare my Spotify Wrapped playlist of 2019 to the one of my boyfriend, Pim. Our taste in music is very different and those lists are representing this difference. I think it’s interesting to compare these two playlists. I’m more into soft pop music, singer-/songwriters, also a little classical music, film music, etc. And Pim is more into Drum & Bass, Rap, Hiphop, etc. But we appreciate each others taste of music increasingly since we've been together. For a research question I would think of how different our music tastes really are. At first glance, they are very different. But maybe there are some underlying similarities. And what those differenses and similarities mean. 




### Pim is dancing a lot

```{r}
pien_2019 <- get_playlist_audio_features('spotify', '37i9dQZF1EtqET0jdwPeZm')
pim_2019 <- get_playlist_audio_features('spotify', '37i9dQZF1Etc8rOydKv4e7')

spotify_wrapped_2019 <- pim_2019 %>% mutate(playlist = "Pim") %>%
  bind_rows(pien_2019 %>% mutate(playlist = "Pien"))

dancing <- spotify_wrapped_2019 %>%                       # Start with awards.
  ggplot(aes(x = danceability,
      y = energy,
      size = loudness,
      colour = factor(mode), label = track.name)) +
  geom_point() +               # Scatter plot.
  geom_rug(size = 0.1) +       # Add 'fringes' to show data distribution.
  facet_wrap(~ playlist) +     # Separate charts per playlist.
  scale_x_continuous(          # Fine-tune the x axis.
    limits = c(0, 1),
    breaks = c(0, 0.50, 1),  # Use grid-lines for quadrants only.
    minor_breaks = NULL      # Remove 'minor' grid-lines.
  ) +
  scale_y_continuous(          # Fine-tune the y axis in the same way.
    limits = c(0, 1),
    breaks = c(0, 0.50, 1),
    minor_breaks = NULL
  ) +
  scale_colour_brewer(         # Use the Color Brewer to choose a palette.
    type = "qual",           # Qualitative set.
    palette = "Paired"       # Name of the palette is 'Paired'.
  ) +
  scale_size_continuous(       # Fine-tune the sizes of each point.
    trans = "exp",           # Use an exp transformation to emphasise loud.
    guide = "none"           # Remove the legend for size.
  ) +
  theme_light() +              # Use a simpler them.
  labs(                        # Make the titles nice.
    x = "Danceability",
    y = "Energy",
    colour = "Mode"
  )




ggplotly(dancing)


```

***

I have some first findings, measured by Spotify, that are interesting: 
The music that Pim listens to is more danceable (M = 0.72, SD = 0.13) than the music I listen to (M = 0.48, SD = 0.19). Also there is a difference in energy. Pim’s playlist is way more energetic (M = 0.68, SD = 0.18) than mine (M = 0.34, SD = 0.25). If we look at the loudness category, Pim’s playlist is louder (M = -6.72, SD = 2.41) than mine (M = -14.1, SD = 8.79). In the figure below, we can see the differences that I discussed above. In Pim's figure, almost all of the points/dots are on the top right. This means, that the the energy and danceability is very high. The loudness is indicated with the size of the points/dots. In my figure, the points/dots are more spread out. But almost all the points/dots are very small. So in the size (loudness), Pim is more diverse. And my figure is more stable. Which is a little strange because my standard deviation is a lot bigger than Pim's. To make this more clear, I made a figure of the loudness. 

### Loudness

```{r fig.width=5, fig.height=8}
loudness <- spotify_wrapped_2019 %>%
  ggplot(aes(x = playlist, y = loudness)) +
  geom_violin()

ggplotly(loudness)
```

***
In the figure we see that Pim is more specific and I am more diverse and stable. This makes more sense. My standard deviation of loudness is large because I listen to more different types of music. A reason for this could be that I have to listen a lot of music for my study, musicology. And this is especially classical music. 

### Acoustic, keys, mode and no speechiness 

```{r}
acoustic <- spotify_wrapped_2019 %>%
  ggplot(aes(x = playlist, y = acousticness)) +
  geom_violin()

ggplotly(acoustic)
```

***

If we look at the acousticness category, there is a difference. My playlist is more acoustic (M = 0.63, SD = 0.36) than the playlist of Pim (M = 0.13, SD = 0.19). Pim listens to music with a higher frequency. In the figure below, we see that I do listen to acoustic music but also to not-acoustic music. On the other hand, Pim doesn't listen to very acoustic music. His figure is the other way around. But in my figure, it is not a huge difference in acousticness, because of the standard deviation. In the figure of Pim, it is very clear that he doesn't listen to acoustic music. Therefore, he has a very small standard deviation.


The keys of our playlists are a lot closer to each other. Pim: (M = 5.43, SD = 3.74), which means in the pitch class F/F#. And mine: (M = 4.54, SD = 3.35), which means in the pitch class E/F. Also the modes are close to each other. Pim: (M = 0.53, SD = 0.5) and mine: (M = 0.57, SD = 0.49). This means that we both listen to major and minor music. But we both have a low valence. We both listen to more negative sounds and sad music. Pim: (M = 0.4, SD = 0.24) and mine: (M = 0.26, SD = 0.19). 

The results of speechiness seems a little extreme to me. Despite the fact that Pim listens to rap, the results are very low (M = 0.14, SD = 0.11). Mine are lower than that, but I don’t listen to rap that often (M = 0.06, SD = 0.06). I would expect that the results of the speechiness in Pim’s playlist would be between 0.33 and 0.66. I don’t think I have to exclude this because it might be an interesting thing to investigate more. 

### Chromagram

```{r}

apocalypse <- 
    get_tidy_audio_analysis('0yc6Gst2xkRu0eMLeRMGCX') %>% 
    select(segments) %>% unnest(segments) %>% 
    select(start, duration, pitches)

apocalypse %>% 
    mutate(pitches = map(pitches, compmus_normalise, 'euclidean')) %>% 
    compmus_gather_chroma %>% 
    ggplot(
        aes(
            x = start + duration / 2, 
            width = duration, 
            y = pitch_class, 
            fill = value)) + 
    geom_tile() +
    labs(x = 'Time (s)', y = NULL, fill = 'Magnitude') +
    theme_minimal()

```

