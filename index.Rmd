---
title:  'Computational Musicology'
author: 'Pien Zwart'
date:   'February--March 2020'
output: 
    flexdashboard::flex_dashboard:
        storyboard: true
        theme: readable
        highlight: monochrome
        
        
        
        
---

```{r setup}
# In order to use these packages, we need to install flexdashboard, plotly, and Cairo.
library(tidyverse)
library(tidymodels)
library(protoclust)
library(ggdendro)
library(heatmaply)
library(plotly)
library(spotifyr)
source('spotify.R')
library(compmus)
```


### Pien Versus Pim

For my corpus, I would like to compare my Spotify Wrapped playlist of 2019 to the one of my boyfriend, Pim. Our taste in music is very different and those lists are representing this difference. 

I think it’s interesting to compare these two playlists. I’m more into soft pop music, singer-/songwriters, also a little classical music, film music, etc. And Pim is more into Drum & Bass, Rap, Hiphop, etc. But we appreciate each others taste of music increasingly since we've been together. For a research question I would think of how different our music tastes really are. At first glance, they are very different. But maybe there are some underlying similarities. And what do these differenses and similarities mean.

The meaning of music is also different. Pim doesn't like to really listen to en think about lyrics of songs. He just want to feel it physically, especially the beat, and dance or focus. I really love to feel music emotionally and think about lyrics. For me, music is a way to express feelings. 

I'm going to look at the similarities and differences in our playlists. First I will compare the whole playlists. By doing that, I will get a clear overview of my corpus. After that, I am going to take a deeper look by comparing specific songs of our playlists. I will be doing this for chroma and timbre, key, tempo and classification.



### Pim Is Dancing A Lot More

```{r}
pien_2019 <- get_playlist_audio_features('spotify', '37i9dQZF1EtqET0jdwPeZm')
pim_2019 <- get_playlist_audio_features('spotify', '37i9dQZF1Etc8rOydKv4e7')

spotify_wrapped_2019 <- pim_2019 %>% mutate(playlist = "Pim") %>%
  bind_rows(pien_2019 %>% mutate(playlist = "Pien"))

dancing <- spotify_wrapped_2019 %>%    # Start with awards.
  mutate(
    mode = ifelse(mode == 0, 'Minor', 'Major')
  ) %>%
  ggplot(aes(x = danceability,
      y = energy,
      size = loudness,
      colour = factor(mode), label = track.name)) +
  geom_point() +               # Scatter plot.
  geom_rug(size = 0.1) +       # Add 'fringes' to show data distribution.
  facet_wrap(~ playlist) +     # Separate charts per playlist.
  scale_x_continuous(          # Fine-tune the x axis.
    limits = c(0, 1),
    breaks = c(0, 0.50, 1),  # Use grid-lines for quadrants only.
    minor_breaks = NULL      # Remove 'minor' grid-lines.
  ) +
  scale_y_continuous(          # Fine-tune the y axis in the same way.
    limits = c(0, 1),
    breaks = c(0, 0.50, 1),
    minor_breaks = NULL
  ) +
  scale_colour_brewer(         # Use the Color Brewer to choose a palette.
    type = "qual",           # Qualitative set.
    palette = "Paired"       # Name of the palette is 'Paired'.
  ) +
  scale_size_continuous(       # Fine-tune the sizes of each point.
    trans = "exp",           # Use an exp transformation to emphasise loud.
    guide = "none"           # Remove the legend for size.
  ) +
  theme_light() +              # Use a simpler them.
  labs(                        # Make the titles nice.
    x = "Danceability",
    y = "Energy",
    colour = "Mode",
    size = "Loudness"
  )




ggplotly(dancing)



```


***
I have some interesting first findings, measured by Spotify.   
The music that Pim listens to is more danceable (M = 0.72, SD = 0.13) than the music I listen to (M = 0.48, SD = 0.19). Also there is a difference in energy. Pim’s playlist is way more energetic (M = 0.68, SD = 0.18) than mine (M = 0.34, SD = 0.25). If we look at the loudness category, Pim’s playlist is louder (M = -6.72, SD = 2.41) than mine (M = -14.1, SD = 8.79). In the figure below, we can see the differences that I discussed above. In Pim's figure, almost all of the points/dots are on the top right. This means, that the the energy and danceability is very high. The loudness is indicated with the size of the points/dots. In my figure, the points/dots are more spread out. But almost all the points/dots are very small. So in the size (loudness), Pim is more diverse. And my figure is more stable. Which I think is a little strange because my standard deviation is a lot bigger than Pim's. To make this more clear, I made a figure of the loudness in the next tab. 

### Pim Is Louder But I Am More Varied

```{r fig.width=5, fig.height=8}
loudness <- spotify_wrapped_2019 %>%
  ggplot(aes(x = playlist, y = loudness)) +
  geom_violin()

ggplotly(loudness)
```

***
In the figure we see that Pim is more specific and I am more diverse and stable. This makes more sense than the other figure. My standard deviation of loudness is large because I listen to more different types of music. A reason for this could be that I have to listen a lot of music for my study, musicology. And this is especially classical music. Therefore, from now on, I will look more at the part where there are more similarities and leave the bottom left side of my playlist as kind of outliers. I will do this because otherwise the differences become really big and not very useful. 

### We Do Have Similarities!


This table shows us a summery of all the Spotify features. In the previous tabs I discussed the difference in Danceability, Energy and Loudness. If we look at the Acousticness, there is also a quit large difference. 

But there are also similarities like the Keys of our playlists. If we look at the results of Pim in this category, it would mean in the pitch class: F/F#. My results for Key would mean in pitch class: E/F. Later in this portfolio, I will take a look at the keys of two specific songs.

Also the modes are close to each other. In the table we see that Pim en I both listen to major, minor and everything in between.

Apparently, Pim en I are both not really into cheerful music. We score pretty low on Valence, especially I am. But I didn't expect that Pim would be pretty close. 

The results of speechiness seems a little extreme to me. Despite the fact that Pim listens to rap, the results are very low. I would expect that the results of the speechiness in Pim’s playlist would be between 0.33 and 0.66. I don’t think I have to exclude this because I think that it won't have impact. It might be an interesting thing to investigate more. But for now I think it is better to focus more on the other features. 

*** 

| Danceability | Mean |  SD  |
|--------------|------|------|
| Pim          | 0.72 | 0.13 |
| Pien         | 0.48 | 0.19 |

|Energy        | Mean |  SD  |
|--------------|------|------|
| Pim          | 0.68 | 0.18 |
| Pien         | 0.34 | 0.25 |

| Loudness     | Mean |  SD  |
|--------------|------|------|
| Pim          | -6.72| 2.41 |
| Pien         | -14.1| 8.79 |

| Acousticness | Mean |  SD  |
|--------------|------|------|
| Pim          | 0.13 | 0.19 |
| Pien         | 0.63 | 0.36 |

|Key           | Mean |  SD  |
|--------------|------|------|
| Pim          | 5.43 | 3.74 |
| Pien         | 4.54 | 3.35 |

| Mode         | Mean |  SD  |
|--------------|------|------|
| Pim          | 0.53 | 0.50 |
| Pien         | 0.57 | 0.49 |

| Speechiness  | Mean |  SD  |
|--------------|------|------|
| Pim          | 0.14 | 0.11 |
| Pien         | 0.06 | 0.06 |

Valence        | Mean |  SD  |
|--------------|------|------|
| Pim          | 0.40 | 0.24 |
| Pien         | 0.26 | 0.19 |

| Liveness     | Mean |  SD  |
|--------------|------|------|
| Pim          | 0.18 | 0.12 |
| Pien         | 0.16 | 0.14 |

| Tempo        | Mean |  SD  |
|--------------|------|------|
| Pim          | 129  | 30.8 |
| Pien         | 113  | 32.2 |

***





### There Are Only Two Songs That Appear In Both Of Our Playlists.

```{r}
apocalypse <- 
    get_tidy_audio_analysis('0yc6Gst2xkRu0eMLeRMGCX') %>% 
    select(segments) %>% unnest(segments) %>% 
    select(start, duration, pitches)
highest <- 
    get_tidy_audio_analysis('3eekarcy7kvN4yt5ZFzltW') %>% 
    select(segments) %>% unnest(segments) %>% 
    select(start, duration, pitches)


apocalypse %>% 
    mutate(pitches = map(pitches, compmus_normalise, 'euclidean')) %>% 
    compmus_gather_chroma %>% 
    ggplot(
        aes(
            x = start + duration / 2, 
            width = duration, 
            y = pitch_class, 
            fill = value)) + 
    geom_tile() +
    labs(x = 'Time (s)', y = NULL, fill = 'Magnitude', title = 'Apocalypse') +
    theme_minimal()


highest %>% 
    mutate(pitches = map(pitches, compmus_normalise, 'euclidean')) %>% 
    compmus_gather_chroma %>% 
    ggplot(
        aes(
            x = start + duration / 2, 
            width = duration, 
            y = pitch_class, 
            fill = value)) + 
    geom_tile() +
    labs(x = 'Time (s)', y = NULL, fill = 'Magnitude', title = 'Highest In The Room') +
    theme_minimal()
```

***
There are only two songs that appear in both our playlists. Those songs are Apocalypse by Cigarettes After Sex and Highest In The Room by Travis Scott. These songs are very very different but they could have something in common. 
In the chromagram of Apocalypse, there are a lot of little blue pieces. Mostly at A, F, D and C through the whole song. In the mittle of the song, there is a blue part at G. 
In the chromagram of Highest In The Room, there are blue sections. This song seems to have more structure. First there is a section with E and F, then a sections between C and D and these sections alternate. At almost the end of the song, it turns to something different. Here is more blue at A#/Bb, G and D. 

### How Are The Structures Of Our Most Danceable Songs?

```{r}

badguy_timbre <- get_tidy_audio_analysis('2Fxmhks0bxGSBdJ92vM42m') %>% 
    compmus_align(sections, segments) %>% 
    select(sections) %>% unnest(sections) %>% 
    mutate(
        pitches = 
            map(segments, 
                compmus_summarise, pitches, 
                method = 'rms', norm = 'euclidean')) %>% 
    mutate(
        timbre = 
            map(segments, 
                compmus_summarise, timbre, 
                method = 'mean'))



badguy_timbre %>% 
    compmus_self_similarity(timbre, 'cosine') %>% 
    ggplot(
        aes(
            x = xstart + xduration / 2, 
            width = xduration,
            y = ystart + yduration / 2,
            height = yduration,
            fill = d)) + 
    geom_tile() +
    coord_fixed() +
    scale_fill_viridis_c(option = 'A', guide = 'none') +
    theme_classic() +
    labs(x = '', y = '', title = 'Bad Guy', subtitle = 'Timbre')



hunnybee_timbre <- get_tidy_audio_analysis('3DPFmwFtV5ElQaTniLOdgk') %>% 
    compmus_align(sections, segments) %>% 
    select(sections) %>% unnest(sections) %>% 
    mutate(
        pitches = 
            map(segments, 
                compmus_summarise, pitches, 
                method = 'rms', norm = 'euclidean')) %>% 
    mutate(
        timbre = 
            map(segments, 
                compmus_summarise, timbre, 
                method = 'mean'))



hunnybee_timbre %>% 
    compmus_self_similarity(timbre, 'cosine') %>% 
    ggplot(
        aes(
            x = xstart + xduration / 2, 
            width = xduration,
            y = ystart + yduration / 2,
            height = yduration,
            fill = d)) + 
    geom_tile() +
    coord_fixed() +
    scale_fill_viridis_c(option = 'A', guide = 'none') +
    theme_classic() +
    labs(x = '', y = '', title = 'Hunnybee', subtitle = 'Timbre')


badguy_chroma <- get_tidy_audio_analysis('2Fxmhks0bxGSBdJ92vM42m') %>% 
    compmus_align(sections, segments) %>% 
    select(sections) %>% unnest(sections) %>% 
    mutate(
        pitches = 
            map(segments, 
                compmus_summarise, pitches, 
                method = 'mean', norm = 'manhattan')) %>% 
    mutate(
        chroma = 
            map(segments, 
                compmus_summarise, timbre, 
                method = 'mean'))



badguy_chroma %>% 
    compmus_self_similarity(chroma, 'manhattan') %>% 
    ggplot(
        aes(
            x = xstart + xduration / 2, 
            width = xduration,
            y = ystart + yduration / 2,
            height = yduration,
            fill = d)) + 
    geom_tile() +
    coord_fixed() +
    scale_fill_viridis_c(option = 'A', guide = 'none') +
    theme_classic() +
    labs(x = '', y = '', title = 'Bad Guy', subtitle = 'Chroma')


hunnybee_chroma <- get_tidy_audio_analysis('3DPFmwFtV5ElQaTniLOdgk') %>% 
    compmus_align(sections, segments) %>% 
    select(sections) %>% unnest(sections) %>% 
    mutate(
        pitches = 
            map(segments, 
                compmus_summarise, pitches, 
                method = 'mean', norm = 'manhattan')) %>% 
    mutate(
        chroma = 
            map(segments, 
                compmus_summarise, timbre, 
                method = 'mean'))



hunnybee_chroma %>% 
    compmus_self_similarity(chroma, 'manhattan') %>% 
    ggplot(
        aes(
            x = xstart + xduration / 2, 
            width = xduration,
            y = ystart + yduration / 2,
            height = yduration,
            fill = d)) + 
    geom_tile() +
    coord_fixed() +
    scale_fill_viridis_c(option = 'A', guide = 'none') +
    theme_classic() +
    labs(x = '', y = '', title = 'Hunnybee', subtitle = 'Chroma')
```

For the timbre and chroma I compare the two most danceable songs in our playlists. In my playlist is 'Bury a Friend' by Billie Eilish the most danceable song. In the playlist of Pim, 'Hunnybee' by Unkown Mortal Orchestra is the most danceable song. I chose these two songs because I think it would be interesting to look if these songs have similarities.

The grams of Bad Guy shows us that there is a structure in the song. And that changes at almost the end of the song. The structure stops. But according to the grams, the beat is considered to be almost the same as before (the purple blocks). Around 150, it turns into a little lighter purple but it's almost the same. 

In the grams of Hunnybee, there is not really a clear structure. There is a beat that continues trough the whole song. That is why the blocks are almost the same color, except the black one. The differences are in the lyrics of the couplets and refrains. Just after 200, something happens. Here is a guitarsolo and after that, there is again the refrain. 

So these songs don't have a lot similarities. They both have a deviation in their structur, but their structures are really different. What they do have in common is that in both songs, there is a beat that continues almost the whole songs. So I think that's why they both score high on danceability. 

### The Keys Of Our Most Energetic Songs

```{r}
circshift <- function(v, n) {if (n == 0) v else c(tail(v, n), head(v, -n))}
                                    
    # C     C#    D     Eb    E     F     F#    G     Ab    A     Bb    B 
major_chord <- 
    c(1,    0,    0,    0,    1,    0,    0,    1,    0,    0,    0,    0)
minor_chord <- 
    c(1,    0,    0,    1,    0,    0,    0,    1,    0,    0,    0,    0)
seventh_chord <- 
    c(1,    0,    0,    0,    1,    0,    0,    1,    0,    0,    1,    0)

major_key <- 
    c(6.35, 2.23, 3.48, 2.33, 4.38, 4.09, 2.52, 5.19, 2.39, 3.66, 2.29, 2.88)
minor_key <-
    c(6.33, 2.68, 3.52, 5.38, 2.60, 3.53, 2.54, 4.75, 3.98, 2.69, 3.34, 3.17)

chord_templates <-
    tribble(
        ~name  , ~template,
        'Gb:7'  , circshift(seventh_chord,  6),
        'Gb:maj', circshift(major_chord,    6),
        'Bb:min', circshift(minor_chord,   10),
        'Db:maj', circshift(major_chord,    1),
        'F:min' , circshift(minor_chord,    5),
        'Ab:7'  , circshift(seventh_chord,  8),
        'Ab:maj', circshift(major_chord,    8),
        'C:min' , circshift(minor_chord,    0),
        'Eb:7'  , circshift(seventh_chord,  3),
        'Eb:maj', circshift(major_chord,    3),
        'G:min' , circshift(minor_chord,    7),
        'Bb:7'  , circshift(seventh_chord, 10),
        'Bb:maj', circshift(major_chord,   10),
        'D:min' , circshift(minor_chord,    2),
        'F:7'   , circshift(seventh_chord,  5),
        'F:maj' , circshift(major_chord,    5),
        'A:min' , circshift(minor_chord,    9),
        'C:7'   , circshift(seventh_chord,  0),
        'C:maj' , circshift(major_chord,    0),
        'E:min' , circshift(minor_chord,    4),
        'G:7'   , circshift(seventh_chord,  7),
        'G:maj' , circshift(major_chord,    7),
        'B:min' , circshift(minor_chord,   11),
        'D:7'   , circshift(seventh_chord,  2),
        'D:maj' , circshift(major_chord,    2),
        'F#:min', circshift(minor_chord,    6),
        'A:7'   , circshift(seventh_chord,  9),
        'A:maj' , circshift(major_chord,    9),
        'C#:min', circshift(minor_chord,    1),
        'E:7'   , circshift(seventh_chord,  4),
        'E:maj' , circshift(major_chord,    4),
        'G#:min', circshift(minor_chord,    8),
        'B:7'   , circshift(seventh_chord, 11),
        'B:maj' , circshift(major_chord,   11),
        'D#:min', circshift(minor_chord,    3))

key_templates <-
    tribble(
        ~name    , ~template,
        'Gb:maj', circshift(major_key,  6),
        'Bb:min', circshift(minor_key, 10),
        'Db:maj', circshift(major_key,  1),
        'F:min' , circshift(minor_key,  5),
        'Ab:maj', circshift(major_key,  8),
        'C:min' , circshift(minor_key,  0),
        'Eb:maj', circshift(major_key,  3),
        'G:min' , circshift(minor_key,  7),
        'Bb:maj', circshift(major_key, 10),
        'D:min' , circshift(minor_key,  2),
        'F:maj' , circshift(major_key,  5),
        'A:min' , circshift(minor_key,  9),
        'C:maj' , circshift(major_key,  0),
        'E:min' , circshift(minor_key,  4),
        'G:maj' , circshift(major_key,  7),
        'B:min' , circshift(minor_key, 11),
        'D:maj' , circshift(major_key,  2),
        'F#:min', circshift(minor_key,  6),
        'A:maj' , circshift(major_key,  9),
        'C#:min', circshift(minor_key,  1),
        'E:maj' , circshift(major_key,  4),
        'G#:min', circshift(minor_key,  8),
        'B:maj' , circshift(major_key, 11),
        'D#:min', circshift(minor_key,  3))


lions_key <- get_tidy_audio_analysis('70GmGE3PnqsvZmyHme6sjI') %>% 
    compmus_align(sections, segments) %>% 
    select(sections) %>% unnest(sections) %>% 
    mutate(
        pitches = 
            map(segments, 
                compmus_summarise, pitches, 
                method = 'mean', norm = 'manhattan'))

lions_key %>% 
    compmus_match_pitch_template(key_templates, 'euclidean', 'manhattan') %>% 
    ggplot(
        aes(x = start + duration / 2, width = duration, y = name, fill = d)) +
    geom_tile() +
    scale_fill_viridis_c(option = 'A', guide = 'none') +
    theme_minimal() +
    labs(x = 'Time (s)', y = '', title = 'Lions Dont Cry', subtitle = 'Keygram')

aguella_key <- get_tidy_audio_analysis('2odYqDHPb6zyBoZjozWOaA') %>% 
    compmus_align(sections, segments) %>% 
    select(sections) %>% unnest(sections) %>% 
    mutate(
        pitches = 
            map(segments, 
                compmus_summarise, pitches, 
                method = 'mean', norm = 'manhattan'))

aguella_key %>% 
    compmus_match_pitch_template(key_templates, 'euclidean', 'manhattan') %>% 
    ggplot(
        aes(x = start + duration / 2, width = duration, y = name, fill = d)) +
    geom_tile() +
    scale_fill_viridis_c(option = 'A', guide = 'none') +
    theme_minimal() +
    labs(x = 'Time (s)', y = '', title = 'Aguella - Original Mix', subtitle = 'Keygram')
```

*** 
Here I made two keygrams of the songs in our playlists with the most energy. In my playlist the most energetic song is Lions Don't Cry by Tim Akkerman and has an energy of 0.93. In Pim's playlist the song with the most energy is Aguella - Original Mix by Sllash & Doppe and has a energy of 0.98. The keygrams are looking very different. Let's take a look.

The keygram of Lions Don't Cry is very clear. It says that almost the whole song Db major and G major are present. Also E minor occurs a lot. There are also a few other keys that are very dark in the gram. I can compare these with the information that Chordify gives about this song. According to Chordify this song is in E minor. Here is the G major right because that is the III. The Db major is a little bit strange. Because in E minor it can be a C#, that is the sixth increased (verhoogde zesde trap). But in Chordify there is a C major and D major, not something in between. But in the E minor key, it is possible. Also in the keygram, the D# minor is really dark. This is the vii. It is not present as chord in the song but for the key it is really logical. One thing that is really noticable is the B. According to Chordify B major and B minor are both present in this song. The B minor more often than the majeur chord. The B major would be logical because that is the fifth/V and this chord should be major. So why the B minor is more common, is a little but strange. Also both the B major and B minor are not really visible in the keygram. So beside the B minor/majeur thing, the key of the song (E minor) is very clear in both the keygram and Chordify. 

The keygram of Aguella is not so clear. I think that this is a hard song to analyze because it is electronic music. In the keygram the B minor and major, F# minor and Ab major are really dark. According to Chordify, this song is in G# minor. The Ab major can be a confusion with G# because that is the same tone. And in chordify there is sometimes a G# major present. The G# minor is in the keygram quite light and that is really strange because the G# minor chords appears a lot in the song. Also according to Chordify, the chord D# minor appears a lot. But in the key G# minor, D# would be the fifth/V and that should be a major chord. But the key of this song (G# minor) is clear. 

But of course these are keygrams and both of them are showing the keys clearly. They don't really show the chords in the songs. The songs don't have a lot in common, only that they are both energetic. I think that the keygram of Lions don't cry is more clear because that song is made with recognizable instruments and Aguella is really electronic and maybe harder to recognize.

### Highest In The Room And Apocalypse Are Really Different In Tempo

```{r}
highest_tempo <- get_tidy_audio_analysis('3eekarcy7kvN4yt5ZFzltW')

highest_tempo %>% 
    tempogram(window_size = 8, hop_size = 1, cyclic = TRUE) %>% 
    ggplot(aes(x = time, y = bpm, fill = power)) + 
    geom_raster() + 
    scale_fill_viridis_c(guide = 'none') +
    labs(x = 'Time (s)', y = 'Tempo (BPM)', title = 'Higest In The Room', subtitle = 'Tempo') +
    theme_classic()


apocalypse_tempo <- get_tidy_audio_analysis('0yc6Gst2xkRu0eMLeRMGCX')

apocalypse_tempo %>% 
    tempogram(window_size = 8, hop_size = 1, cyclic = TRUE) %>% 
    ggplot(aes(x = time, y = bpm, fill = power)) + 
    geom_raster() + 
    scale_fill_viridis_c(guide = 'none') +
    labs(x = 'Time (s)', y = 'Tempo (BPM)', title = 'Apocalypse', subtitle = 'Tempo') +
    theme_classic()

```

*** 
For the tempograms, I picked the only two songs that appear in both our playlists. Because these two songs seems to be very different, I chose to work these two out in tempograms. And maybe I can come to a conclusion why these two particular songs are the only two that are in both of our playlists. 

You can see in the tempograms, that Highest In The Room by Travis Scott scores really high in tempo. It is almost the whole song about 155 BPM and sometimes it even reaches the 160 BPM. 

In the tempogram of Apocalypse by Cigarettes After Sex that the this song is really slow. It is about 90 BPM. Sometimes it is almost 100 BPM and sometimes it is even on the lowest point of the tempogram, 80 BPM. 

If we look back at the table in my similarities, we see that the mean of my tempo is 113 BPM with a standard deviation of 32.2. The song Highest In The Room is really fast for my playlist and Apocalypse is almost on the bottom edge. The mean of Pim's tempo is 129 BPM with a standard deviation of 30.8. Here are both songs really on the edge of the tempo range. Highest In The Room is on the top and Apocalypse on the bottom.  

So I can conclude here that the two songs differs a lot in tempo. It is almost the exact opposite. Still we both really like these songs. Maybe our tastes in genres are not that different, only the levels in the genres. 


### My Songs Are More Spread Out


```{r}
pien <- 
    get_playlist_audio_features('spotify', '37i9dQZF1EtqET0jdwPeZm') %>% 
    slice(1:20) %>% 
    add_audio_analysis
pim <- 
    get_playlist_audio_features('spotify', '37i9dQZF1Etc8rOydKv4e7') %>% 
    slice(1:20) %>% 
    add_audio_analysis

spotify_2019 <- pim %>% mutate(playlist = "Pim") %>%
  bind_rows(pien %>% mutate(playlist = "Pien")) %>% 
    mutate(playlist = factor(playlist)) %>% 
    mutate(
        segments = 
            map2(segments, key, compmus_c_transpose)) %>% 
    mutate(
        pitches = 
            map(segments, 
                compmus_summarise, pitches, 
                method = 'mean', norm = 'manhattan'),
        timbre =
            map(
                segments,
                compmus_summarise, timbre,
                method = 'mean')) %>% 
    mutate(pitches = map(pitches, compmus_normalise, 'clr')) %>% 
    mutate_at(vars(pitches, timbre), map, bind_rows) %>% 
    unnest(cols = c(pitches, timbre))

spotify_2019_class <- 
    recipe(playlist ~
               danceability +
               energy +
               loudness +
               speechiness +
               acousticness +
               instrumentalness +
               liveness +
               valence +
               tempo +
               duration +
               C + `C#|Db` + D + `D#|Eb` +
               E + `F` + `F#|Gb` + G +
               `G#|Ab` + A + `A#|Bb` + B +
               c01 + c02 + c03 + c04 + c05 + c06 +
               c07 + c08 + c09 + c10 + c11 + c12,
           data = spotify_2019) %>% 
    step_center(all_predictors()) %>%
    step_scale(all_predictors()) %>%
    # step_range(all_predictors()) %>% 
    prep(spotify_2019) %>% 
    juice

spotify_2019_cv <- spotify_2019_class %>% vfold_cv(5)

spotify_2019_knn <- 
    nearest_neighbor(mode = 'classification', neighbors = 1) %>% 
    set_engine('kknn')
predict_knn <- function(split)
    fit(spotify_2019_knn, playlist ~ ., data = analysis(split)) %>% 
    predict(assessment(split), type = 'class') %>%
    bind_cols(assessment(split))



spotify_2019_cv %>% 
    mutate(pred = map(splits, predict_knn)) %>% unnest(pred) %>% 
    conf_mat(truth = playlist, estimate = .pred_class) %>% 
    autoplot(type = 'heatmap')


spotify_2019_logistic <- 
    logistic_reg(mode = 'classification') %>% 
    set_engine('glm')
predict_logistic <- function(split)
    fit(spotify_2019_logistic, playlist ~ ., data = analysis(split)) %>% 
    predict(assessment(split), type = 'class') %>%
    bind_cols(assessment(split))

spotify_2019_multinom <- 
    multinom_reg(mode = 'classification', penalty = 0.1) %>% 
    set_engine('glmnet')
predict_multinom <- function(split)
    fit(spotify_2019_multinom, playlist ~ ., data = analysis(split)) %>% 
    predict(assessment(split), type = 'class') %>%
    bind_cols(assessment(split))




spotify_2019_tree <- 
    decision_tree(mode = 'classification') %>%
    set_engine('C5.0')
predict_tree <- function(split)
    fit(spotify_2019_tree, playlist ~ ., data = analysis(split)) %>% 
    predict(assessment(split), type = 'class') %>%
    bind_cols(assessment(split))


spotify_2019 %>%
    ggplot(aes(x = c01, y = c02, colour = playlist, size = liveness)) +
    geom_point(alpha = 0.8) +
    scale_color_brewer(type = 'qual', palette = 'Accent') +
    labs(
        x = 'Timbre Component 1', 
        y = 'Timbre Component 2', 
        size = 'Liveness', 
        colour = 'Playlist'
    )


```

***
I made a confusion matrix of our playlists. You can see that the differences are pretty clear and that the predictions are about 75% correct. But you also can see here, that Pim is clearer than I am. The first time I ran this, it varied only two (or one) songs but the prediction that a song belongs in Pim's playlist when it really belongs to mine, occured more than the prediction that a song belongs in my playlist when in fact it belongs in Pim's. So I would conclude out this figure that I' more flexible than Pim is. 

Now it changed and it looks like the prediction that a song belongs to my playlist when it actually belongs to Pims, occurs more. The first outcomes are more logic to me, and matches better with the plot. 

In the plot you see the songs that belongs in my playlist, are more spread out over the whole plot. The songs of Pim's playlist, are very close to each other. So that it is more clear which songs belongs in Pim's playlist and that it is more vague which songs belongs to mine. 

Like we saw in the table of all Spotify features, there is almost the same difference in liveness in our playlists. 

Note: my confusion matrix changes everytime I run the code. So maybe my information differs from the matrix... But the plot is there to be a stable factor.



### The Conclusion Of My Corpus

For now, I can conclude that my boyfriend and I really have different tastes in music genres. But sometimes there are a few similarities. I think that it would be really interesting to look in a few years, what the differences are then. It is a pity that I don't have these wrapped up lists from the other years that we've been together. Because it would be interesting to compare all those lists. 

I think that I listen to a lot more different genres than Pim does. I've focused in this portfolio on the most danceable and energetic songs, and the two songs that appear in both our playlists. I haven't focused on the slowest songs because those diffences are really big. 

In the results, we've seen that Pim's playlist is more danceable and mine is more acoustic. I've also discussed our similarities and we both listen to major and minor but not really cheerful music. The mean in key of my playlist is E/F. And de mean in key in the playlist of Pim is F/F#. If we compare this to the keygrams I made, it is right. The keygram of Lions Don't Cry from my playlist is in E minor. And the keygram of Aguella from Pim's playlist is in G# minor. My most energetic song is a little bit closer to my mean than Pim's most energetic song.

If we look at the tempi, the songs that I compared are really opossites but they're presenting the differences of tempo in both of our playists and similarities in our playlists. The differences in our music tastes are not in tempo, but more in loudness, danceability and energy. 

In the classification visualizations we saw that I also listen to music that could appear in the playlist of Pim. But not the other way around. I think this is because I started to listen to hiphop more last year, and that's because of Pim. 

What would be interesting is to look at the results from this year and the results of upcoming years. If I keep doing this in the upcoming years, I can compare all the results and see if there is a specific movement. Are we growing to each other? Or do we keep our own taste in music? Or maybe our music tastes will melt together. I think it would be interesting to keep looking at this and see the differences of every year.   



